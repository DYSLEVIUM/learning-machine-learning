{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Machine Learning Tutorial 9 - Ensemble Learning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyML86J2Vtakdvd62qfVsO3w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vfGysE7BBmMV"},"source":["# Ensemble Learning\n","\n","It uses multiple machine learning models or multiple set of models for the same algorithm which try to make a better prediction.\n","\n","Ensemble Learning model works by training different models on the same dataset and makes prediction iindividually and once the prediction is made then these results are combines with some statistical methods to get final prediction\n","\n","In one sentence we can explain like this there is a dataset where multiple algorithms are trained on the same dataset and then finally predictions are made based on the outcomes of the individual machine learning algorithms.\n","\n","Let me explain this with an example of cricket team, in cricket team or any other team every few players are specialized in some fields(batting, fast bowling, fielding, keeping, … etc). In the same way every algorithm has its own feature set. There are multiple algorithms and they are specialized in some way so once we combine all of these algorithms it’s easy to get the final predictions.\n","\n","<center><img src=\"https://machinelearningmastery.com/wp-content/uploads/2020/11/Bagging-Ensemble.png\"/></center>"]},{"cell_type":"markdown","metadata":{"id":"TagKArfbCtoq"},"source":["# Need of Ensemble\n","\n","Every model has its own strength and weakness. If we combine multiple models it will help us to hide weakness of individual models sothat we can cover weakness of others.\n","\n","It creates some errors, The error emerging from any machine model can be broken down into three components mathematically. Following are these component:\n","\n","> Bias\n","> Variance\n","> Irreducible error\n","\n","To understand these errors have a look at the following figure:\n","\n","<center><img src=\"https://jason-chen-1992.weebly.com/uploads/1/0/8/5/108557741/bias-and-variance_orig.png\"/></center>\n","\n","\n","Bias error is useful to quantify how much on an average are the predicted values different from the actual value.\n","\n","Variance on the other side quantifies how are the prediction made on the same observation different from each other.\n","\n","<center><img src=\"https://kgptalkie.com/wp-content/uploads/2020/08/image-126.png\"/></center>\n","\n","Now we will try to understand bias - variance trade off from the following figure.\n","By increasing model complexity, total error will decrease till some point and then it will start to increase. W need to select optimum model complexity to get less error.\n","\n","For low complexity model : high bias and low variance\n","For high complexity model : low bias and high variance\n","\n","If you are getting high bias then you have a fair chance to increase model complexity. And otherside it you are getting high variance, you need to decrease model complexity that’s how any machine learning algorithm works.\n"]},{"cell_type":"markdown","metadata":{"id":"VyluB2yrEhRv"},"source":["# Types of Ensemble Learning\n","\n","Basic Ensemble Techniques\n","  - Max Voting\n","\n","  - Averaging\n","\n","  - Weighted Average\n","\n","Advanced Ensemble Techniques\n","  - Stacking\n","\n","  - Blending\n","\n","  - Bagging\n","\n","  - Boosting\n","\n","\n","## Algorithms based on Bagging\n","  - Bagging meta-estimator\n","\n","  - Random Forest\n","\n","## Boosting Algorithms\n","  - AdaBoost\n","\n","  - GBM\n","\n","  - XGB\n","\n","  - Light GBM\n","\n","  - CatBoost\n","\n","## Max Voting\n","\n","The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point.\n","\n","## Averaging\n","\n","Similar to the max voting technique, multiple predictions are made for each data point in averaging.\n","\n","## Weighted Average\n","\n","This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction.\n","\n","## Bagging\n","Bagging is also known as Bootstrapping. It is a sampling technique in which we create subsets of observations from the original dataset, with replacement. The size of the subsets is the same as the size of the original set.\n","\n","  - Combining predictions that belong to the same type.\n","  - Aim to decrease variance, not bias.\n","  - Different training data subsets are randomly drawn with replacement from the entire training dataset.\n","\n","To explain bagging Random Forest(below figure) is the best example.\n","\n","<center><img src=\"https://kgptalkie.com/wp-content/uploads/2020/08/image-127.png\"/></center>\n","\n","It creates multiple subsets like decision tree and it makes a prediction for each decision tree then if random forest is classifier it will take max voting otherwise if it is a regressor it will take avearge from each of these subset of the trees .\n","\n","## Boosting\n","Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model.\n","Letâ€™s understand the way boosting works in the below steps.\n","\n","- Combining predictions that belong to the different types.\n","- Aim to decrease bias, not variance.\n","- Models are weighted according to their performance.\n","\n","Let’s now understand boosting from the following figure: \n","\n","<center><img src=\"https://kgptalkie.com/wp-content/uploads/2020/08/image-128.png\"/></center>\n","\n","At first we have our original dataset ,our first algorithm creates a plane there for that we have SVM classifier, Random Forest classifier, etc and it found out that there are some errors in the plane . To rectife that errors , we will train other model and after this again we will train other model which identifies errors.\n","Finally, we combine all three models together which perfectly classify our original dataset."]},{"cell_type":"markdown","metadata":{"id":"VTO8gRgOG7iq"},"source":["# Algorithms Implimentation in sklearn\n","\n","- Bagging\n","  - Random Forest\n","\n","- Boosting\n","  - XGBosst\n","  - AdaBoost\n","  - Gradient Boosting\n","\n","**Random Forest** is another ensemble machine learning algorithm that follows the bagging technique\n","\n","**XGBoost (extreme Gradient Boosting)** is an advanced implementation of the gradient boosting algorithm\n","\n","**Adaptive boosting or AdaBoost** is one of the simplest boosting algorithms\n","\n","**Gradient Boosting or GBM** is another ensemble machine learning algorithm that works for both regression and classification problems"]},{"cell_type":"code","metadata":{"id":"aqzkUVpsA9gl","executionInfo":{"status":"ok","timestamp":1629739040483,"user_tz":-345,"elapsed":1096,"user":{"displayName":"Pushpakant Behera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOdt2zcFZx2xm2pfBJ77r7EiGiL-teZ09k-ejI4A=s64","userId":"01383014390996239087"}}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","sns.set()"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"mB8l11anBBvH","executionInfo":{"status":"ok","timestamp":1629740843515,"user_tz":-345,"elapsed":3,"user":{"displayName":"Pushpakant Behera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOdt2zcFZx2xm2pfBJ77r7EiGiL-teZ09k-ejI4A=s64","userId":"01383014390996239087"}}},"source":["from sklearn import datasets, metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWpyN1uhBDAm","executionInfo":{"status":"ok","timestamp":1629740727439,"user_tz":-345,"elapsed":491,"user":{"displayName":"Pushpakant Behera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOdt2zcFZx2xm2pfBJ77r7EiGiL-teZ09k-ejI4A=s64","userId":"01383014390996239087"}}},"source":["cancer = datasets.load_breast_cancer()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BamEVExDHfjJ","executionInfo":{"status":"ok","timestamp":1629740740413,"user_tz":-345,"elapsed":682,"user":{"displayName":"Pushpakant Behera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOdt2zcFZx2xm2pfBJ77r7EiGiL-teZ09k-ejI4A=s64","userId":"01383014390996239087"}},"outputId":"a1fc8e9c-dc99-4f40-b84e-4c74bbe0bdc2"},"source":["print(cancer.DESCR)"],"execution_count":5,"outputs":[{"output_type":"stream","text":[".. _breast_cancer_dataset:\n","\n","Breast cancer wisconsin (diagnostic) dataset\n","--------------------------------------------\n","\n","**Data Set Characteristics:**\n","\n","    :Number of Instances: 569\n","\n","    :Number of Attributes: 30 numeric, predictive attributes and the class\n","\n","    :Attribute Information:\n","        - radius (mean of distances from center to points on the perimeter)\n","        - texture (standard deviation of gray-scale values)\n","        - perimeter\n","        - area\n","        - smoothness (local variation in radius lengths)\n","        - compactness (perimeter^2 / area - 1.0)\n","        - concavity (severity of concave portions of the contour)\n","        - concave points (number of concave portions of the contour)\n","        - symmetry \n","        - fractal dimension (\"coastline approximation\" - 1)\n","\n","        The mean, standard error, and \"worst\" or largest (mean of the three\n","        largest values) of these features were computed for each image,\n","        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n","        13 is Radius SE, field 23 is Worst Radius.\n","\n","        - class:\n","                - WDBC-Malignant\n","                - WDBC-Benign\n","\n","    :Summary Statistics:\n","\n","    ===================================== ====== ======\n","                                           Min    Max\n","    ===================================== ====== ======\n","    radius (mean):                        6.981  28.11\n","    texture (mean):                       9.71   39.28\n","    perimeter (mean):                     43.79  188.5\n","    area (mean):                          143.5  2501.0\n","    smoothness (mean):                    0.053  0.163\n","    compactness (mean):                   0.019  0.345\n","    concavity (mean):                     0.0    0.427\n","    concave points (mean):                0.0    0.201\n","    symmetry (mean):                      0.106  0.304\n","    fractal dimension (mean):             0.05   0.097\n","    radius (standard error):              0.112  2.873\n","    texture (standard error):             0.36   4.885\n","    perimeter (standard error):           0.757  21.98\n","    area (standard error):                6.802  542.2\n","    smoothness (standard error):          0.002  0.031\n","    compactness (standard error):         0.002  0.135\n","    concavity (standard error):           0.0    0.396\n","    concave points (standard error):      0.0    0.053\n","    symmetry (standard error):            0.008  0.079\n","    fractal dimension (standard error):   0.001  0.03\n","    radius (worst):                       7.93   36.04\n","    texture (worst):                      12.02  49.54\n","    perimeter (worst):                    50.41  251.2\n","    area (worst):                         185.2  4254.0\n","    smoothness (worst):                   0.071  0.223\n","    compactness (worst):                  0.027  1.058\n","    concavity (worst):                    0.0    1.252\n","    concave points (worst):               0.0    0.291\n","    symmetry (worst):                     0.156  0.664\n","    fractal dimension (worst):            0.055  0.208\n","    ===================================== ====== ======\n","\n","    :Missing Attribute Values: None\n","\n","    :Class Distribution: 212 - Malignant, 357 - Benign\n","\n","    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n","\n","    :Donor: Nick Street\n","\n","    :Date: November, 1995\n","\n","This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n","https://goo.gl/U2Uwz2\n","\n","Features are computed from a digitized image of a fine needle\n","aspirate (FNA) of a breast mass.  They describe\n","characteristics of the cell nuclei present in the image.\n","\n","Separating plane described above was obtained using\n","Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n","Construction Via Linear Programming.\" Proceedings of the 4th\n","Midwest Artificial Intelligence and Cognitive Science Society,\n","pp. 97-101, 1992], a classification method which uses linear\n","programming to construct a decision tree.  Relevant features\n","were selected using an exhaustive search in the space of 1-4\n","features and 1-3 separating planes.\n","\n","The actual linear program used to obtain the separating plane\n","in the 3-dimensional space is that described in:\n","[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n","Programming Discrimination of Two Linearly Inseparable Sets\",\n","Optimization Methods and Software 1, 1992, 23-34].\n","\n","This database is also available through the UW CS ftp server:\n","\n","ftp ftp.cs.wisc.edu\n","cd math-prog/cpo-dataset/machine-learn/WDBC/\n","\n",".. topic:: References\n","\n","   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n","     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n","     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n","     San Jose, CA, 1993.\n","   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n","     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n","     July-August 1995.\n","   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n","     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n","     163-171.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OmgCDZr8HhWw","executionInfo":{"status":"ok","timestamp":1629740774554,"user_tz":-345,"elapsed":394,"user":{"displayName":"Pushpakant Behera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOdt2zcFZx2xm2pfBJ77r7EiGiL-teZ09k-ejI4A=s64","userId":"01383014390996239087"}}},"source":["X = cancer.data\n","y = cancer.target"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"beyZvRdbHrFY","executionInfo":{"status":"ok","timestamp":1629740782177,"user_tz":-345,"elapsed":397,"user":{"displayName":"Pushpakant Behera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOdt2zcFZx2xm2pfBJ77r7EiGiL-teZ09k-ejI4A=s64","userId":"01383014390996239087"}},"outputId":"194a90dc-edd6-42dd-f6f1-865d05f9fcad"},"source":["X.shape, y.shape"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((569, 30), (569,))"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DTkRkSMKHs8g","executionInfo":{"status":"ok","timestamp":1629740852694,"user_tz":-345,"elapsed":381,"user":{"displayName":"Pushpakant Behera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOdt2zcFZx2xm2pfBJ77r7EiGiL-teZ09k-ejI4A=s64","userId":"01383014390996239087"}},"outputId":"c14d743f-df97-448b-bead-7f2e3830cb35"},"source":["scaler=StandardScaler()\n","X_scaled=scaler.fit_transform(X)\n","X_scaled"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.09706398, -2.07333501,  1.26993369, ...,  2.29607613,\n","         2.75062224,  1.93701461],\n","       [ 1.82982061, -0.35363241,  1.68595471, ...,  1.0870843 ,\n","        -0.24388967,  0.28118999],\n","       [ 1.57988811,  0.45618695,  1.56650313, ...,  1.95500035,\n","         1.152255  ,  0.20139121],\n","       ...,\n","       [ 0.70228425,  2.0455738 ,  0.67267578, ...,  0.41406869,\n","        -1.10454895, -0.31840916],\n","       [ 1.83834103,  2.33645719,  1.98252415, ...,  2.28998549,\n","         1.91908301,  2.21963528],\n","       [-1.80840125,  1.22179204, -1.81438851, ..., -1.74506282,\n","        -0.04813821, -0.75120669]])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"qDYSG4ExH2Fg","executionInfo":{"status":"ok","timestamp":1629740883355,"user_tz":-345,"elapsed":390,"user":{"displayName":"Pushpakant Behera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOdt2zcFZx2xm2pfBJ77r7EiGiL-teZ09k-ejI4A=s64","userId":"01383014390996239087"}}},"source":["from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n","import xgboost as xgb"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"gTq1Gjx1IFo-","executionInfo":{"status":"ok","timestamp":1629740991331,"user_tz":-345,"elapsed":393,"user":{"displayName":"Pushpakant Behera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOdt2zcFZx2xm2pfBJ77r7EiGiL-teZ09k-ejI4A=s64","userId":"01383014390996239087"}}},"source":["X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=1, stratify=y)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYMWweO4If_j","executionInfo":{"status":"ok","timestamp":1629741232111,"user_tz":-345,"elapsed":647,"user":{"displayName":"Pushpakant Behera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOdt2zcFZx2xm2pfBJ77r7EiGiL-teZ09k-ejI4A=s64","userId":"01383014390996239087"}}},"source":["rfc=RandomForestClassifier(n_estimators=200, random_state=1)\n","adc=AdaBoostClassifier(n_estimators=200, random_state=1, learning_rate=0.01)\n","gbc=GradientBoostingClassifier(n_estimators=200, random_state=1, learning_rate=0.01)\n","xgb_clf=xgb.XGBClassifier(n_estimators=200, learning_rate=0.01, random_state=1)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F1bzAFXaI0LP","executionInfo":{"status":"ok","timestamp":1629741233960,"user_tz":-345,"elapsed":1852,"user":{"displayName":"Pushpakant Behera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOdt2zcFZx2xm2pfBJ77r7EiGiL-teZ09k-ejI4A=s64","userId":"01383014390996239087"}},"outputId":"1ec090a4-5051-4945-ce32-d66703f4d231"},"source":["rfc.fit(X_train, y_train)\n","abc.fit(X_train, y_train)\n","gbc.fit(X_train, y_train)\n","xgb_clf.fit(X_train, y_train)"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","              colsample_bynode=1, colsample_bytree=1, gamma=0,\n","              learning_rate=0.01, max_delta_step=0, max_depth=3,\n","              min_child_weight=1, missing=None, n_estimators=200, n_jobs=1,\n","              nthread=None, objective='binary:logistic', random_state=1,\n","              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","              silent=None, subsample=1, verbosity=1)"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHL7jDEhJApf","executionInfo":{"status":"ok","timestamp":1629741234357,"user_tz":-345,"elapsed":401,"user":{"displayName":"Pushpakant Behera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOdt2zcFZx2xm2pfBJ77r7EiGiL-teZ09k-ejI4A=s64","userId":"01383014390996239087"}},"outputId":"0475e46f-5dee-48ce-de96-5fe42204163d"},"source":["print('Random Forest', rfc.score(X_test, y_test))\n","print('AdaBoost', abc.score(X_test, y_test))\n","print('Gradient Boost', gbc.score(X_test, y_test))\n","print('XGBoost', xgb_clf.score(X_test, y_test))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Random Forest 0.9473684210526315\n","AdaBoost 0.9473684210526315\n","Gradient Boost 0.9736842105263158\n","XGBoost 0.956140350877193\n"],"name":"stdout"}]}]}